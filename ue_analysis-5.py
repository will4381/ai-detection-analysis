# -*- coding: utf-8 -*-
"""ue-analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SW6dAIca-HoYSEwV4Iq4MC0bQ0-hfgrB
"""

# Commented out IPython magic to ensure Python compatibility.
# Install required packages
# %pip install openai requests datasets scikit-learn tqdm networkx gensim spacy wordcloud textstat nltk matplotlib seaborn pandas numpy textblob optuna lightgbm xgboost shap scipy wordcloud

# Standard Libraries
import json
import os
import re
import time
from collections import Counter, defaultdict

# Third-Party Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import networkx as nx
import spacy
from spacy import displacy
from spacy.tokens import Doc
from textstat import flesch_kincaid_grade, flesch_reading_ease
from textblob import TextBlob
from tqdm import tqdm
from wordcloud import WordCloud

# Google Colab
from google.colab import drive
from google.colab import userdata

# OpenAI
from openai import OpenAI

# Requests
import requests

# Datasets
from datasets import load_dataset

# Natural Language Processing
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
from gensim.models import Word2Vec

# Scikit-Learn
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import RFE
from sklearn.impute import SimpleImputer
from sklearn.inspection import permutation_importance, PartialDependenceDisplay
from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, ElasticNetCV
from sklearn.manifold import TSNE
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.neural_network import MLPRegressor
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.svm import SVR

# Statistical Analysis
from scipy import stats
from scipy.stats import ttest_rel, pearsonr, spearmanr

# Machine Learning Libraries
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
import optuna

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

# Mount Google Drive
drive.mount('/content/drive')

# Define API keys (replace with actual keys)
undetectable_api_key = userdata.get('undetectable_secret')
gptzero_api_key = userdata.get('gptzero_api_key')
openai_api_key = userdata.get('openai_secret')

# Authenticate OpenAI
client = OpenAI(api_key=openai_api_key)

def remove_markdown(text):
    if not text:
        return text

    # Remove headers
    text = re.sub(r'^\s*#.*$', '', text, flags=re.MULTILINE)

    # Remove bold and italic
    text = re.sub(r'\*\*|__|\*|_', '', text)

    # Remove links
    text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)

    # Remove inline code
    text = re.sub(r'`([^`]+)`', r'\1', text)

    # Remove code blocks
    text = re.sub(r'```[\s\S]*?```', '', text)

    # Remove blockquotes
    text = re.sub(r'^\s*>.*$', '', text, flags=re.MULTILINE)

    # Remove horizontal rules
    text = re.sub(r'^\s*[-*_]{3,}\s*$', '', text, flags=re.MULTILINE)

    # Remove extra newlines
    text = re.sub(r'\n{3,}', '\n\n', text)

    return text.strip()

# Load the dataset
ds = load_dataset("yahma/alpaca-cleaned")
instructions = ds['train']['instruction']

def generate_ai_content(prompt):
    try:
        completion = client.chat.completions.create(
            model="gpt-4",  # or "gpt-3.5-turbo" depending on your preference
            messages=[
                {"role": "system", "content": "You are a helpful assistant who does not use markdown in their responses."},
                {"role": "user", "content": prompt}
            ]
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        print(f"Error generating content for prompt '{prompt}': {e}")
        return None

# Generate AI content for the first 10 examples
print("Generating AI content for the first 10 examples:")
original_texts = []
for prompt in tqdm(instructions[:10]):
    content = generate_ai_content(prompt)
    if content:
        content = remove_markdown(content)  # Remove markdown
    original_texts.append(content)
    print(f"Prompt: {prompt}")
    print(f"Generated content (markdown removed): {content[:100]}...")  # Print first 100 characters
    print()

def retrieve_from_undetectable(document_id, max_attempts=10, delay=10):
    for attempt in range(max_attempts):
        url = "https://api.undetectable.ai/document"
        payload = json.dumps({"id": document_id})
        headers = {
            'api-key': undetectable_api_key,
            'Content-Type': 'application/json'
        }
        response = requests.post(url, headers=headers, data=payload)
        result = response.json()

        if result.get('status') == 'done':
            return result

        print(f"Attempt {attempt + 1}: Text still processing. Waiting {delay} seconds...")
        time.sleep(delay)

    return None

def submit_to_undetectable(content):
    url = "https://api.undetectable.ai/submit"
    payload = json.dumps({
        "content": content,
        "readability": "University",
        "purpose": "General Writing",
        "strength": "More Human"
    })
    headers = {
        'api-key': undetectable_api_key,
        'Content-Type': 'application/json'
    }
    response = requests.post(url, headers=headers, data=payload)
    return response.json()

print("\nProcessing texts with Undetectable.ai:")
undetectable_texts = []
for idx, text in enumerate(tqdm(original_texts)):
    print(f"\nProcessing text {idx + 1}:")
    if text:
        response_data = submit_to_undetectable(text)
        print(f"Submission response: {response_data}")
        if 'id' in response_data:
            document_id = response_data['id']
            result = retrieve_from_undetectable(document_id)
            if result:
                undetectable_texts.append(result.get('output', None))
                print(f"Original (no markdown): {text[:50]}...")
                print(f"Undetectable: {result.get('output', 'N/A')[:50]}...")
            else:
                print("Failed to retrieve processed text after multiple attempts")
                undetectable_texts.append(None)
        else:
            print(f"Failed to get document ID. Response: {response_data}")
            undetectable_texts.append(None)
    else:
        print("Skipping due to missing original text")
        undetectable_texts.append(None)
    time.sleep(8.0)  # Respect rate limit

def get_ai_detection_scores(text):
    try:
        url = "https://api.gptzero.me/v2/predict/text"
        payload = {
            "document": text,
            "version": "2024-04-04"
        }
        headers = {
            "x-api-key": gptzero_api_key,
            "Content-Type": "application/json",
            "Accept": "application/json"
        }
        response = requests.post(url, json=payload, headers=headers)
        data = response.json()
        if data['documents']:
            return data['documents'][0]['class_probabilities']
        else:
            return {'ai': 0, 'human': 0, 'mixed': 0}
    except Exception as e:
        print(f"Error getting detection scores: {e}")
        return {'ai': 0, 'human': 0, 'mixed': 0}

# Get AI detection scores
print("\nGetting AI detection scores:")
original_scores = []
undetectable_scores = []
for idx, (original, undetectable) in enumerate(tqdm(zip(original_texts, undetectable_texts))):
    print(f"\nProcessing pair {idx + 1}:")
    if original and undetectable:
        print(f"Original text (first 50 chars): {original[:50]}...")
        print(f"Undetectable text (first 50 chars): {undetectable[:50]}...")

        orig_score = get_ai_detection_scores(original)
        undet_score = get_ai_detection_scores(undetectable)

        original_scores.append(orig_score)
        undetectable_scores.append(undet_score)

        print(f"Original scores: {orig_score}")
        print(f"Undetectable scores: {undet_score}")
    else:
        print("Skipping this pair due to missing text.")

    print("-" * 50)

print(f"\nTotal pairs processed: {len(original_scores)}")
print(f"Total original scores: {len(original_scores)}")
print(f"Total undetectable scores: {len(undetectable_scores)}")

# Save results to CSV
results = []
for idx, (original_text, undetectable_text, original_score, undetectable_score) in enumerate(zip(original_texts, undetectable_texts, original_scores, undetectable_scores)):
    results.append({
        "prompt": instructions[idx],
        "original_text": original_text,
        "undetectable_text": undetectable_text,
        "original_scores": original_score,
        "undetectable_scores": undetectable_score
    })

# Save to CSV
df = pd.DataFrame(results)
csv_path = '/content/drive/My Drive/ai_text_analysis_results.csv'
df.to_csv(csv_path, index=False)
print(f"Results saved to {csv_path}")

nlp = spacy.load("en_core_web_sm")

def preprocess_text(text):
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    tokens = word_tokenize(text.lower())
    return [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]

def analyze_texts(original_texts, undetectable_texts):
    preprocessed_original = [preprocess_text(text) for text in original_texts if text is not None]
    preprocessed_undetectable = [preprocess_text(text) for text in undetectable_texts if text is not None]

    if not preprocessed_original or not preprocessed_undetectable:
        print("Warning: All texts are None in either original or undetectable set.")
        return None, [], [], None, None, None

    tfidf = TfidfVectorizer()
    tfidf_original = tfidf.fit_transform([' '.join(text) for text in preprocessed_original])
    tfidf_undetectable = tfidf.transform([' '.join(text) for text in preprocessed_undetectable])

    similarities = cosine_similarity(tfidf_original, tfidf_undetectable)

    model = Word2Vec(preprocessed_original + preprocessed_undetectable, vector_size=100, window=5, min_count=1, workers=4)

    changes = []
    semantic_shifts = []
    for orig, undet in zip(preprocessed_original, preprocessed_undetectable):
        if orig and undet:
            orig_set = set(orig)
            undet_set = set(undet)
            removed = orig_set - undet_set
            added = undet_set - orig_set
            changes.append((removed, added))

            try:
                orig_vec = np.mean([model.wv[word] for word in orig if word in model.wv], axis=0)
                undet_vec = np.mean([model.wv[word] for word in undet if word in model.wv], axis=0)
                semantic_shifts.append(cosine_similarity([orig_vec], [undet_vec])[0][0])
            except Exception as e:
                print(f"Error calculating semantic shift: {e}")
                semantic_shifts.append(None)
        else:
            changes.append((set(), set()))
            semantic_shifts.append(None)

    return similarities, changes, semantic_shifts, model, tfidf_original, tfidf_undetectable

def analyze_syntactic_complexity(texts):
    complexity_scores = []
    for text in texts:
        if text is None:
            complexity_scores.append({
                'tree_depth': None,
                'num_clauses': None,
                'fk_grade': None,
                'flesch_ease': None
            })
        else:
            try:
                doc = nlp(text)
                tree_depth = max(token.head.i - token.i for token in doc)
                num_clauses = len([sent for sent in doc.sents])
                complexity_scores.append({
                    'tree_depth': tree_depth,
                    'num_clauses': num_clauses,
                    'fk_grade': flesch_kincaid_grade(text),
                    'flesch_ease': flesch_reading_ease(text)
                })
            except Exception as e:
                print(f"Error processing text: {e}")
                complexity_scores.append({
                    'tree_depth': None,
                    'num_clauses': None,
                    'fk_grade': None,
                    'flesch_ease': None
                })
    return pd.DataFrame(complexity_scores)

def analyze_named_entity_changes(original_texts, undetectable_texts):
    def get_entities(text):
        if text is None:
            return set()
        doc = nlp(text)
        return set((ent.text, ent.label_) for ent in doc.ents)

    entity_changes = []
    for orig, undet in zip(original_texts, undetectable_texts):
        if orig is None or undet is None:
            entity_changes.append({
                'removed': set(),
                'added': set(),
                'change_ratio': 0
            })
        else:
            orig_entities = get_entities(orig)
            undet_entities = get_entities(undet)
            removed = orig_entities - undet_entities
            added = undet_entities - orig_entities
            entity_changes.append({
                'removed': removed,
                'added': added,
                'change_ratio': (len(removed) + len(added)) / (len(orig_entities) + 1e-10)
            })
    return entity_changes

def analyze_sentiment_changes(original_texts, undetectable_texts):
    sentiment_changes = []
    for orig, undet in zip(original_texts, undetectable_texts):
        if orig is None or undet is None:
            sentiment_changes.append(0)
        else:
            orig_sentiment = TextBlob(orig).sentiment.polarity
            undet_sentiment = TextBlob(undet).sentiment.polarity
            sentiment_changes.append(undet_sentiment - orig_sentiment)
    return sentiment_changes

def text_coherence(text):
    if text is None:
        return 0
    doc = nlp(text)
    coherence_score = sum(token.is_alpha and not token.is_stop for token in doc) / len(doc)
    return coherence_score

def lexical_diversity(text):
    if text is None:
        return 0
    tokens = word_tokenize(text.lower())
    return len(set(tokens)) / len(tokens) if tokens else 0

def analyze_topic_changes(tfidf_original, tfidf_undetectable):
    def extract_topics(tfidf_matrix, num_topics=5):
        svd = TruncatedSVD(n_components=num_topics)
        svd.fit(tfidf_matrix)
        return svd.components_

    orig_topics = extract_topics(tfidf_original)
    undet_topics = extract_topics(tfidf_undetectable)

    topic_similarities = cosine_similarity(orig_topics, undet_topics)
    return topic_similarities

def pad_or_truncate(some_list, target_len):
    return some_list[:target_len] + [None]*(target_len - len(some_list))

def calculate_correlations(feature, ai_score_changes):
    valid_indices = [i for i, v in enumerate(feature) if v is not None]
    feature_valid = [feature[i] for i in valid_indices]
    ai_score_valid = [ai_score_changes[i] for i in valid_indices]

    if len(feature_valid) < 2:
        return 0, 1, len(feature_valid)

    correlation, p_value = spearmanr(feature_valid, ai_score_valid)
    return correlation, p_value, len(feature_valid)

def objective(trial, model_class, X, y):
    if model_class == RandomForestRegressor:
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'max_depth': trial.suggest_int('max_depth', 5, 30),
            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10)
        }
    elif model_class == GradientBoostingRegressor:
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1.0),
            'max_depth': trial.suggest_int('max_depth', 3, 10)
        }
    elif model_class == XGBRegressor:
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1.0),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
            'subsample': trial.suggest_uniform('subsample', 0.5, 1.0)
        }
    elif model_class == LGBMRegressor:
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1.0),
            'num_leaves': trial.suggest_int('num_leaves', 20, 100),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100)
        }
    else:
        raise ValueError("Unsupported model class")

    model = model_class(**params)
    score = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')
    return -np.mean(score)

# Main analysis
similarities, changes, semantic_shifts, word2vec_model, tfidf_original, tfidf_undetectable = analyze_texts(original_texts, undetectable_texts)
orig_complexity = analyze_syntactic_complexity(original_texts)
undet_complexity = analyze_syntactic_complexity(undetectable_texts)
entity_changes = analyze_named_entity_changes(original_texts, undetectable_texts)
sentiment_changes = analyze_sentiment_changes(original_texts, undetectable_texts)
topic_similarities = analyze_topic_changes(tfidf_original, tfidf_undetectable)

orig_coherence = [text_coherence(text) for text in original_texts]
undet_coherence = [text_coherence(text) for text in undetectable_texts]
coherence_changes = np.array(undet_coherence) - np.array(orig_coherence)

orig_diversity = [lexical_diversity(text) for text in original_texts]
undet_diversity = [lexical_diversity(text) for text in undetectable_texts]
diversity_changes = np.array(undet_diversity) - np.array(orig_diversity)

ai_score_changes = np.array([score['ai'] for score in undetectable_scores]) - np.array([score['ai'] for score in original_scores])

semantic_shifts = pad_or_truncate(semantic_shifts, len(ai_score_changes))
undet_fk_grade = pad_or_truncate(undet_complexity['fk_grade'].tolist(), len(ai_score_changes))
orig_fk_grade = pad_or_truncate(orig_complexity['fk_grade'].tolist(), len(ai_score_changes))
syntactic_changes = [u - o if u is not None and o is not None else None for u, o in zip(undet_fk_grade, orig_fk_grade)]
entity_change_ratios = pad_or_truncate([change['change_ratio'] for change in entity_changes], len(ai_score_changes))
sentiment_changes = pad_or_truncate(sentiment_changes, len(ai_score_changes))
coherence_changes = pad_or_truncate(coherence_changes.tolist(), len(ai_score_changes))
diversity_changes = pad_or_truncate(diversity_changes.tolist(), len(ai_score_changes))

correlations = {
    "Semantic Shift": calculate_correlations(semantic_shifts, ai_score_changes),
    "Syntactic Complexity Change": calculate_correlations(syntactic_changes, ai_score_changes),
    "Named Entity Change Ratio": calculate_correlations(entity_change_ratios, ai_score_changes),
    "Sentiment Change": calculate_correlations(sentiment_changes, ai_score_changes),
    "Coherence Change": calculate_correlations(coherence_changes, ai_score_changes),
    "Lexical Diversity Change": calculate_correlations(diversity_changes, ai_score_changes),
}

print("\nCorrelations with AI Detection Score Changes:")
for feature, (correlation, p_value, n) in correlations.items():
    if n < 2:
        print(f"{feature}: Not enough data (only {n} valid samples)")
    else:
        print(f"{feature}: correlation = {correlation:.4f}, p-value = {p_value:.4f} (n = {n})")

feature_values = {
    "Semantic Shift": semantic_shifts,
    "Syntactic Complexity Change": syntactic_changes,
    "Named Entity Change Ratio": entity_change_ratios,
    "Sentiment Change": sentiment_changes,
    "Coherence Change": coherence_changes,
    "Lexical Diversity Change": diversity_changes
}

plt.figure(figsize=(12, 8))
features = list(correlations.keys())
correlation_values = [corr for corr, _, _ in correlations.values()]

sns.barplot(x=correlation_values, y=features)
plt.title("Correlation of Features with AI Detection Score Changes")
plt.xlabel("Spearman Correlation Coefficient")
plt.axvline(x=0, color='r', linestyle='--')
plt.tight_layout()
plt.savefig('/content/drive/My Drive/ai_text_analysis_correlations.png')
plt.show()

top_correlations = sorted(correlations.items(), key=lambda x: abs(x[1][0]), reverse=True)[:3]

fig, axes = plt.subplots(1, 3, figsize=(20, 6))
for i, (feature, (correlation, _, _)) in enumerate(top_correlations):
    sns.scatterplot(x=feature_values[feature], y=ai_score_changes, ax=axes[i])
    axes[i].set_title(f"{feature} vs AI Score Change")
    axes[i].set_xlabel(feature)
    axes[i].set_ylabel("AI Detection Score Change")

plt.tight_layout()
plt.savefig('/content/drive/My Drive/ai_text_analysis_top_correlations.png')
plt.show()

# Define feature names
feature_names = ["Semantic Shift", "Syntactic Complexity Change", "Named Entity Change Ratio",
                 "Sentiment Change", "Coherence Change", "Lexical Diversity Change"]

valid_indices = [i for i in range(len(ai_score_changes)) if all(arr[i] is not None for arr in [semantic_shifts, syntactic_changes, entity_change_ratios, sentiment_changes, coherence_changes, diversity_changes])]

X = np.column_stack([
    [semantic_shifts[i] for i in valid_indices],
    [syntactic_changes[i] for i in valid_indices],
    [entity_change_ratios[i] for i in valid_indices],
    [sentiment_changes[i] for i in valid_indices],
    [coherence_changes[i] for i in valid_indices],
    [diversity_changes[i] for i in valid_indices]
])

y = [ai_score_changes[i] for i in valid_indices]

# Feature selection
def select_features(X, y):
    selector = RFE(estimator=RandomForestRegressor(n_estimators=100, random_state=42), n_features_to_select=10)
    selector = selector.fit(X, y)
    return X[:, selector.support_], selector.support_

X_selected, feature_mask = select_features(X, y)
selected_feature_names = [name for name, selected in zip(feature_names, feature_mask) if selected]

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)

# Define models
models = {
    'Linear Regression': make_pipeline(StandardScaler(), LinearRegression()),
    'Lasso': make_pipeline(StandardScaler(), LassoCV(cv=5)),
    'Ridge': make_pipeline(StandardScaler(), RidgeCV(cv=5)),
    'ElasticNet': make_pipeline(StandardScaler(), ElasticNetCV(cv=5)),
    'Random Forest': RandomForestRegressor(random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(random_state=42),
    'XGBoost': XGBRegressor(random_state=42),
    'LightGBM': LGBMRegressor(random_state=42),
    'SVR': make_pipeline(StandardScaler(), SVR()),
    'Neural Network': make_pipeline(StandardScaler(), MLPRegressor(random_state=42, max_iter=1000))
}

# Optimize and train models
results = {}
for name, model in models.items():
    if name in ['Random Forest', 'Gradient Boosting', 'XGBoost', 'LightGBM']:
        study = optuna.create_study(direction='minimize')
        study.optimize(lambda trial: objective(trial, type(model), X_train, y_train), n_trials=100)
        best_params = study.best_params
        model.set_params(**best_params)

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    results[name] = {
        'r2': r2_score(y_test, y_pred),
        'mse': mean_squared_error(y_test, y_pred),
        'mae': mean_absolute_error(y_test, y_pred)
    }
    print(f"{name}: R2 = {results[name]['r2']:.4f}, MSE = {results[name]['mse']:.4f}, MAE = {results[name]['mae']:.4f}")

# Ensemble model
base_models = [(name, model) for name, model in models.items() if name in ['Random Forest', 'Gradient Boosting', 'XGBoost', 'LightGBM']]
ensemble = StackingRegressor(estimators=base_models, final_estimator=RidgeCV())
ensemble.fit(X_train, y_train)
y_pred_ensemble = ensemble.predict(X_test)
results['Ensemble'] = {
    'r2': r2_score(y_test, y_pred_ensemble),
    'mse': mean_squared_error(y_test, y_pred_ensemble),
    'mae': mean_absolute_error(y_test, y_pred_ensemble)
}
print(f"Ensemble: R2 = {results['Ensemble']['r2']:.4f}, MSE = {results['Ensemble']['mse']:.4f}, MAE = {results['Ensemble']['mae']:.4f}")

# Select best model
best_model_name = max(results, key=lambda x: results[x]['r2'])
best_model = models[best_model_name] if best_model_name != 'Ensemble' else ensemble

# Feature importance
if hasattr(best_model, 'feature_importances_'):
    importances = best_model.feature_importances_
elif hasattr(best_model, 'coef_'):
    importances = np.abs(best_model.coef_)
else:
    importances = permutation_importance(best_model, X_test, y_test, n_repeats=10, random_state=42).importances_mean

# Visualize feature importances
plt.figure(figsize=(10, 6))
sns.barplot(x=importances, y=selected_feature_names)
plt.title(f"Feature Importances - {best_model_name}")
plt.tight_layout()
plt.savefig('/content/drive/My Drive/ai_text_analysis_feature_importances.png')
plt.show()

# SHAP values
explainer = shap.Explainer(best_model, X_train)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test, plot_type="bar", feature_names=selected_feature_names)
plt.savefig('/content/drive/My Drive/ai_text_analysis_shap_summary.png')
plt.show()

# Partial dependence plots
fig, ax = plt.subplots(figsize=(12, 4))
PartialDependenceDisplay.from_estimator(best_model, X_test, features=range(3), feature_names=selected_feature_names, ax=ax)
plt.tight_layout()
plt.savefig('/content/drive/My Drive/ai_text_analysis_partial_dependence.png')
plt.show()

# Residual analysis
y_pred = best_model.predict(X_test)
residuals = y_test - y_pred

plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_pred, y=residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.title('Residual Plot')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.savefig('/content/drive/My Drive/ai_text_analysis_residual_plot.png')
plt.show()

# QQ plot
fig, ax = plt.subplots(figsize=(10, 6))
stats.probplot(residuals, dist="norm", plot=ax)
ax.set_title("Q-Q plot")
plt.savefig('/content/drive/My Drive/ai_text_analysis_qq_plot.png')
plt.show()

# Save comprehensive results
comprehensive_results = []
for idx in range(len(X)):
    result = {
        "prompt": instructions[idx],
        "original_text": original_texts[idx],
        "undetectable_text": undetectable_texts[idx],
        "original_ai_score": original_scores[idx]['ai'],
        "undetectable_ai_score": undetectable_scores[idx]['ai'],
        "ai_score_change": ai_score_changes[idx],
    }
    for name, value in zip(selected_feature_names, X_selected[idx]):
        result[name] = value
    result["predicted_ai_score_change"] = best_model.predict(X_selected[idx].reshape(1, -1))[0]
    comprehensive_results.append(result)

comprehensive_df = pd.DataFrame(comprehensive_results)
comprehensive_csv_path = '/content/drive/My Drive/ai_text_analysis_comprehensive_results.csv'
comprehensive_df.to_csv(comprehensive_csv_path, index=False)
print(f"Comprehensive results saved to {comprehensive_csv_path}")

# Summary of findings
print("\nSummary of Findings:")
print(f"1. Best performing model: {best_model_name}")
print(f"2. Best model R-squared: {results[best_model_name]['r2']:.4f}")
print(f"3. Best model Mean Squared Error: {results[best_model_name]['mse']:.4f}")
print(f"4. Best model Mean Absolute Error: {results[best_model_name]['mae']:.4f}")
print(f"5. Top correlated feature with AI detection score changes: {top_correlations[0][0]}")
print("6. Feature importances:")
for name, importance in zip(selected_feature_names, importances):
    print(f"   - {name}: {importance:.4f}")
print("7. Visualizations saved:")
print("   - Correlation plot: ai_text_analysis_correlations.png")
print("   - Top correlations scatter plot: ai_text_analysis_top_correlations.png")
print("   - Feature importances plot: ai_text_analysis_feature_importances.png")
print("   - SHAP summary plot: ai_text_analysis_shap_summary.png")
print("   - Partial dependence plot: ai_text_analysis_partial_dependence.png")
print("   - Residual plot: ai_text_analysis_residual_plot.png")
print("   - Q-Q plot: ai_text_analysis_qq_plot.png")
print(f"8. Comprehensive results saved to: {comprehensive_csv_path}")