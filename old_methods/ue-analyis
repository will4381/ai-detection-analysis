# OLD METHOD
# Old method which combined data preparation and statistical analysis. Originally from a Colab notebook, with help from Claude Sonnett 3.5.
# Features showed no correlation between AI detection score, as well as low R values.

# pip install openai requests datasets scikit-learn tqdm networkx gensim spacy wordcloud textstat nltk matplotlib seaborn pandas numpy textblob optuna lightgbm xgboost shap scipy wordcloud

# Standard Libraries
import json
import os
import re
import time
from collections import Counter, defaultdict
import sys
import warnings
import logging

# Third-Party Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import networkx as nx
import spacy
from spacy import displacy
from spacy.tokens import Doc
from textstat import flesch_kincaid_grade, flesch_reading_ease
from textblob import TextBlob
from tqdm import tqdm
from wordcloud import WordCloud

# Google Colab
from google.colab import drive
from google.colab import userdata

# OpenAI
from openai import OpenAI

# Requests
import requests

# Datasets
from datasets import load_dataset

# Natural Language Processing
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
from gensim.models import Word2Vec

# Scikit-Learn
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import RFE
from sklearn.impute import SimpleImputer
from sklearn.inspection import permutation_importance, PartialDependenceDisplay
from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, ElasticNetCV
from sklearn.manifold import TSNE
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.neural_network import MLPRegressor
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.svm import SVR
from sklearn.covariance import EllipticEnvelope

# Statistical Analysis
from scipy import stats
from scipy.stats import ttest_rel, pearsonr, spearmanr

# Machine Learning Libraries
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
import optuna

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

# Mount Google Drive
drive.mount('/content/drive')

# Define API keys (replace with actual keys)
undetectable_api_key = userdata.get('undetectable_secret')
gptzero_api_key = userdata.get('gptzero_api_key')
openai_api_key = userdata.get('openai_secret')

# Authenticate OpenAI
client = OpenAI(api_key=openai_api_key)

def remove_markdown(text):
    if not text:
        return text

    # Remove headers
    text = re.sub(r'^\s*#.*$', '', text, flags=re.MULTILINE)

    # Remove bold and italic
    text = re.sub(r'\*\*|__|\*|_', '', text)

    # Remove links
    text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)

    # Remove inline code
    text = re.sub(r'`([^`]+)`', r'\1', text)

    # Remove code blocks
    text = re.sub(r'```[\s\S]*?```', '', text)

    # Remove blockquotes
    text = re.sub(r'^\s*>.*$', '', text, flags=re.MULTILINE)

    # Remove horizontal rules
    text = re.sub(r'^\s*[-*_]{3,}\s*$', '', text, flags=re.MULTILINE)

    # Remove extra newlines
    text = re.sub(r'\n{3,}', '\n\n', text)

    return text.strip()

# Only use if generations .json file does not exist or you wish to create a new one

# Load the dataset
ds = load_dataset("yahma/alpaca-cleaned")
instructions = ds['train']['instruction']

max_tokens = 256
total_tokens_used = 0
max_total_tokens = 1000000  # Equivalent to 97,000 words (97000 / 0.75)

def generate_ai_content(prompt):
    global total_tokens_used
    try:
        completion = client.chat.completions.create(
            model="gpt-4",  # or "gpt-3.5-turbo" depending on your preference
            messages=[
                {"role": "system", "content": "You are a helpful assistant who does not use markdown in their responses."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=max_tokens
        )
        content = completion.choices[0].message.content.strip()
        tokens_used = len(content.split())  # Rough estimate
        total_tokens_used += tokens_used
        return content, tokens_used
    except Exception as e:
        print(f"Error generating content for prompt '{prompt}': {e}")
        return None, 0

print("Generating AI content:")
original_texts = []
for prompt in tqdm(instructions[:10]):
    if total_tokens_used >= max_total_tokens:
        print(f"Reached token limit. Generated {len(original_texts)} examples.")
        break

    content, tokens = generate_ai_content(prompt)
    if content:
        content = remove_markdown(content)  # Remove markdown
        original_texts.append(content)
        print(f"Prompt: {prompt}")
        print(f"Generated content (markdown removed): {content[:100]}...")  # Print first 100 characters
        print(f"Tokens used: {tokens}")
        print(f"Total tokens used so far: {total_tokens_used}")
        print()
    else:
        print(f"Skipped prompt due to error: {prompt}")
        print()

print(f"Total examples generated: {len(original_texts)}")
print(f"Estimated total tokens used: {total_tokens_used}")

# Save generations to a .json file to load later.

# Define the save path
save_path = "/content/drive/My Drive/ai_generations_test1.json"

def save_progress(data, path):
    with open(path, 'w') as f:
        json.dump(data, f)

try:
    # Check if original_texts is defined and has content
    if 'original_texts' in globals() and original_texts:
        save_progress(original_texts, save_path)
        estimated_words = total_tokens_used * 0.75  # Estimated word count assuming 1 token equals ~3/4 words
        print(f"Total examples generated: {len(original_texts)}")
        print(f"Estimated total tokens used: {total_tokens_used}")
        print(f"Estimated words: {estimated_words}")
        print(f"Progress saved to {save_path}")
    else:
        print("No generated content found to save.")
except Exception as e:
    print(f"Error while saving progress: {e}")

# Only use to load already created generations .json file

# Define the save path
save_path = "/content/drive/My Drive/ai_generations_original.json"

# Load the dataset for prompts
ds = load_dataset("yahma/alpaca-cleaned")
instructions = ds['train']['instruction']

# Load existing generated data
try:
    with open(save_path, 'r') as f:
        original_texts = json.load(f)
    print(f"Loaded {len(original_texts)} examples from {save_path}")

    # If the loaded data is not a list, try to extract it from a dictionary
    if isinstance(original_texts, dict):
        if 'original_texts' in original_texts:
            original_texts = original_texts['original_texts']
        else:
            print("Warning: Unexpected data format. Using all loaded data as original texts.")

    # Ensure original_texts is a list
    if not isinstance(original_texts, list):
        raise TypeError("Loaded data is not in the expected format (list or dictionary with 'original_texts' key)")

except FileNotFoundError:
    print(f"No existing data found at {save_path}. Please generate content first.")
    original_texts = []
except json.JSONDecodeError:
    print(f"Error decoding JSON from {save_path}. Please check the file integrity.")
    original_texts = []
except TypeError as e:
    print(f"Error in data format: {e}")
    original_texts = []

print(f"Total examples: {len(original_texts)}")

# Print the first 10 examples or all if less than 10
print("\nFirst 10 examples (or all if less than 10):")
if len(original_texts) == 0:
    print("No examples to display. Please generate content first.")
else:
    for i, text in enumerate(original_texts[:10]):
        print(f"\nExample {i + 1}:")
        if i < len(instructions):
            print(f"Prompt: {instructions[i]}")
        else:
            print("Prompt: Not available")
        print(f"Generated text: {text[:100]}...")  # Print first 100 characters of generated text

# If no data was loaded, provide instructions on how to generate content
if len(original_texts) == 0:
    print("\nTo generate content, please run the content generation code first.")

def retrieve_from_undetectable(document_id, max_attempts=10, delay=5):
    for attempt in range(max_attempts):
        url = "https://api.undetectable.ai/document"
        payload = json.dumps({"id": document_id})
        headers = {
            'api-key': undetectable_api_key,
            'Content-Type': 'application/json'
        }
        response = requests.post(url, headers=headers, data=payload)
        result = response.json()

        if result.get('status') == 'done':
            return result

        print(f"Attempt {attempt + 1}: Text still processing. Waiting {delay} seconds...")
        time.sleep(delay)

    return None

def submit_to_undetectable(content):
    url = "https://api.undetectable.ai/submit"
    payload = json.dumps({
        "content": content,
        "readability": "University",
        "purpose": "General Writing",
        "strength": "More Human"
    })
    headers = {
        'api-key': undetectable_api_key,
        'Content-Type': 'application/json'
    }
    response = requests.post(url, headers=headers, data=payload)
    return response.json()

print("\nProcessing texts with Undetectable.ai:")
undetectable_texts = []
for idx, text in enumerate(tqdm(original_texts)):
    print(f"\nProcessing text {idx + 1}:")
    if text:
        response_data = submit_to_undetectable(text)
        print(f"Submission response: {response_data}")
        if 'id' in response_data:
            document_id = response_data['id']
            result = retrieve_from_undetectable(document_id)
            if result:
                undetectable_texts.append(result.get('output', None))
                print(f"Original (no markdown): {text[:50]}...")
                print(f"Undetectable: {result.get('output', 'N/A')[:50]}...")
            else:
                print("Failed to retrieve processed text after multiple attempts")
                undetectable_texts.append(None)
        else:
            print(f"Failed to get document ID. Response: {response_data}")
            undetectable_texts.append(None)
    else:
        print("Skipping due to missing original text")
        undetectable_texts.append(None)
    time.sleep(8.0)  # 3 requests per second and to perform successful retrieval on first try

def save_undetectable_responses(responses, save_path):
    with open(save_path, 'w') as f:
        json.dump(responses, f)
    print(f"Undetectable responses saved to {save_path}")

# Define the save path
save_path = "/content/drive/My Drive/undetectable_responses_original.json"

# Save the undetectable responses
save_undetectable_responses(undetectable_texts, save_path)

print(f"Saved {len(undetectable_texts)} undetectable responses.")

def get_ai_detection_scores(text):
    try:
        url = "https://api.gptzero.me/v2/predict/text"
        payload = {
            "document": text,
            "version": "2024-04-04"
        }
        headers = {
            "x-api-key": gptzero_api_key,
            "Content-Type": "application/json",
            "Accept": "application/json"
        }
        response = requests.post(url, json=payload, headers=headers)
        data = response.json()
        if data['documents']:
            return data['documents'][0]['class_probabilities']
        else:
            return {'ai': 0, 'human': 0, 'mixed': 0}
    except Exception as e:
        print(f"Error getting detection scores: {e}")
        return {'ai': 0, 'human': 0, 'mixed': 0}

# Get AI detection scores
print("\nGetting AI detection scores:")
original_scores = []
undetectable_scores = []
for idx, (original, undetectable) in enumerate(tqdm(zip(original_texts, undetectable_texts))):
    print(f"\nProcessing pair {idx + 1}:")
    if original and undetectable:
        print(f"Original text (first 50 chars): {original[:50]}...")
        print(f"Undetectable text (first 50 chars): {undetectable[:50]}...")

        orig_score = get_ai_detection_scores(original)
        undet_score = get_ai_detection_scores(undetectable)

        original_scores.append(orig_score)
        undetectable_scores.append(undet_score)

        print(f"Original scores: {orig_score}")
        print(f"Undetectable scores: {undet_score}")
    else:
        print("Skipping this pair due to missing text.")

    print("-" * 50)

print(f"\nTotal pairs processed: {len(original_scores)}")
print(f"Total original scores: {len(original_scores)}")
print(f"Total undetectable scores: {len(undetectable_scores)}")

# Save results to CSV
results = []
for idx, (original_text, undetectable_text, original_score, undetectable_score) in enumerate(zip(original_texts, undetectable_texts, original_scores, undetectable_scores)):
    results.append({
        "prompt": instructions[idx],
        "original_text": original_text,
        "undetectable_text": undetectable_text,
        "original_scores": original_score,
        "undetectable_scores": undetectable_score
    })

# Save to CSV
df = pd.DataFrame(results)
csv_path = '/content/drive/My Drive/ai_text_analysis_results_original.csv'
df.to_csv(csv_path, index=False)
print(f"Results saved to {csv_path}")

# Suppress warnings
warnings.filterwarnings("ignore")

# Suppress optuna logging
optuna.logging.set_verbosity(optuna.logging.WARNING)

# Suppress LightGBM logging
logging.getLogger('lightgbm').setLevel(logging.ERROR)

nlp = spacy.load("en_core_web_sm")

def preprocess_text(text):
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    tokens = word_tokenize(text.lower())
    return [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]

def analyze_texts(original_texts, undetectable_texts):
    preprocessed_original = [preprocess_text(text) for text in original_texts if text is not None]
    preprocessed_undetectable = [preprocess_text(text) for text in undetectable_texts if text is not None]

    if not preprocessed_original or not preprocessed_undetectable:
        print("Warning: All texts are None in either original or undetectable set.")
        return None, [], [], None, None, None

    tfidf = TfidfVectorizer()
    tfidf_original = tfidf.fit_transform([' '.join(text) for text in preprocessed_original])
    tfidf_undetectable = tfidf.transform([' '.join(text) for text in preprocessed_undetectable])

    similarities = cosine_similarity(tfidf_original, tfidf_undetectable)

    model = Word2Vec(preprocessed_original + preprocessed_undetectable, vector_size=100, window=5, min_count=1, workers=4)

    changes = []
    semantic_shifts = []
    for orig, undet in zip(preprocessed_original, preprocessed_undetectable):
        if orig and undet:
            orig_set = set(orig)
            undet_set = set(undet)
            removed = orig_set - undet_set
            added = undet_set - orig_set
            changes.append((removed, added))

            try:
                orig_vec = np.mean([model.wv[word] for word in orig if word in model.wv], axis=0)
                undet_vec = np.mean([model.wv[word] for word in undet if word in model.wv], axis=0)
                semantic_shifts.append(cosine_similarity([orig_vec], [undet_vec])[0][0])
            except Exception as e:
                print(f"Error calculating semantic shift: {e}")
                semantic_shifts.append(None)
        else:
            changes.append((set(), set()))
            semantic_shifts.append(None)

    return similarities, changes, semantic_shifts, model, tfidf_original, tfidf_undetectable

def analyze_syntactic_complexity(texts):
    complexity_scores = []
    for text in texts:
        if text is None:
            complexity_scores.append({
                'tree_depth': None,
                'num_clauses': None,
                'fk_grade': None,
                'flesch_ease': None
            })
        else:
            try:
                doc = nlp(text)
                tree_depth = max(token.head.i - token.i for token in doc)
                num_clauses = len([sent for sent in doc.sents])
                complexity_scores.append({
                    'tree_depth': tree_depth,
                    'num_clauses': num_clauses,
                    'fk_grade': flesch_kincaid_grade(text),
                    'flesch_ease': flesch_reading_ease(text)
                })
            except Exception as e:
                print(f"Error processing text: {e}")
                complexity_scores.append({
                    'tree_depth': None,
                    'num_clauses': None,
                    'fk_grade': None,
                    'flesch_ease': None
                })
    return pd.DataFrame(complexity_scores)

def analyze_named_entity_changes(original_texts, undetectable_texts):
    def get_entities(text):
        if text is None:
            return set()
        doc = nlp(text)
        return set((ent.text, ent.label_) for ent in doc.ents)

    entity_changes = []
    for orig, undet in zip(original_texts, undetectable_texts):
        if orig is None or undet is None:
            entity_changes.append({
                'removed': set(),
                'added': set(),
                'change_ratio': 0
            })
        else:
            orig_entities = get_entities(orig)
            undet_entities = get_entities(undet)
            removed = orig_entities - undet_entities
            added = undet_entities - orig_entities
            entity_changes.append({
                'removed': removed,
                'added': added,
                'change_ratio': (len(removed) + len(added)) / (len(orig_entities) + 1e-10)
            })
    return entity_changes

def analyze_sentiment_changes(original_texts, undetectable_texts):
    sentiment_changes = []
    for orig, undet in zip(original_texts, undetectable_texts):
        if orig is None or undet is None:
            sentiment_changes.append(0)
        else:
            orig_sentiment = TextBlob(orig).sentiment.polarity
            undet_sentiment = TextBlob(undet).sentiment.polarity
            sentiment_changes.append(undet_sentiment - orig_sentiment)
    return sentiment_changes

def text_coherence(text):
    if text is None:
        return 0
    doc = nlp(text)
    coherence_score = sum(token.is_alpha and not token.is_stop for token in doc) / len(doc)
    return coherence_score

def lexical_diversity(text):
    if text is None:
        return 0
    tokens = word_tokenize(text.lower())
    return len(set(tokens)) / len(tokens) if tokens else 0

def analyze_topic_changes(tfidf_original, tfidf_undetectable):
    def extract_topics(tfidf_matrix, num_topics=5):
        svd = TruncatedSVD(n_components=num_topics)
        svd.fit(tfidf_matrix)
        return svd.components_

    orig_topics = extract_topics(tfidf_original)
    undet_topics = extract_topics(tfidf_undetectable)

    topic_similarities = cosine_similarity(orig_topics, undet_topics)
    return topic_similarities

def pad_or_truncate(some_list, target_len):
    return some_list[:target_len] + [None]*(target_len - len(some_list))

def calculate_correlations(feature, ai_score_changes):
    valid_indices = [i for i, v in enumerate(feature) if v is not None and not np.isnan(v)]
    feature_valid = [feature[i] for i in valid_indices]
    ai_score_valid = [ai_score_changes[i] for i in valid_indices]

    if len(feature_valid) < 2:
        return 0, 1, len(feature_valid)

    correlation, p_value = spearmanr(feature_valid, ai_score_valid)
    return correlation, p_value, len(feature_valid)

def objective(trial, model, X, y):
    if isinstance(model, Pipeline):
        model_class = type(model.named_steps['model'])
        param_prefix = 'model__'
    else:
        model_class = type(model)
        param_prefix = ''

    if model_class in [RandomForestRegressor, Pipeline]:
        params = {
            f'{param_prefix}n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            f'{param_prefix}max_depth': trial.suggest_int('max_depth', 5, 30),
            f'{param_prefix}min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
            f'{param_prefix}min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10)
        }
    elif model_class == GradientBoostingRegressor:
        params = {
            f'{param_prefix}n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            f'{param_prefix}learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1.0),
            f'{param_prefix}max_depth': trial.suggest_int('max_depth', 3, 10)
        }
    elif model_class == XGBRegressor:
        params = {
            f'{param_prefix}n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            f'{param_prefix}learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1.0),
            f'{param_prefix}max_depth': trial.suggest_int('max_depth', 3, 10),
            f'{param_prefix}min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
            f'{param_prefix}subsample': trial.suggest_uniform('subsample', 0.5, 1.0)
        }
    elif model_class == LGBMRegressor:
        params = {
            f'{param_prefix}n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            f'{param_prefix}learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1.0),
            f'{param_prefix}num_leaves': trial.suggest_int('num_leaves', 20, 100),
            f'{param_prefix}max_depth': trial.suggest_int('max_depth', 3, 10),
            f'{param_prefix}min_child_samples': trial.suggest_int('min_child_samples', 5, 100)
        }
    else:
        raise ValueError("Unsupported model class")

    model.set_params(**params)
    score = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')
    return -np.mean(score)

# Main analysis
similarities, changes, semantic_shifts, word2vec_model, tfidf_original, tfidf_undetectable = analyze_texts(original_texts, undetectable_texts)
orig_complexity = analyze_syntactic_complexity(original_texts)
undet_complexity = analyze_syntactic_complexity(undetectable_texts)
entity_changes = analyze_named_entity_changes(original_texts, undetectable_texts)
sentiment_changes = analyze_sentiment_changes(original_texts, undetectable_texts)
topic_similarities = analyze_topic_changes(tfidf_original, tfidf_undetectable)

orig_coherence = [text_coherence(text) for text in original_texts]
undet_coherence = [text_coherence(text) for text in undetectable_texts]
coherence_changes = np.array(undet_coherence) - np.array(orig_coherence)

orig_diversity = [lexical_diversity(text) for text in original_texts]
undet_diversity = [lexical_diversity(text) for text in undetectable_texts]
diversity_changes = np.array(undet_diversity) - np.array(orig_diversity)

ai_score_changes = np.array([score['ai'] for score in undetectable_scores]) - np.array([score['ai'] for score in original_scores])

semantic_shifts = pad_or_truncate(semantic_shifts, len(ai_score_changes))
undet_fk_grade = pad_or_truncate(undet_complexity['fk_grade'].tolist(), len(ai_score_changes))
orig_fk_grade = pad_or_truncate(orig_complexity['fk_grade'].tolist(), len(ai_score_changes))
syntactic_changes = [u - o if u is not None and o is not None else None for u, o in zip(undet_fk_grade, orig_fk_grade)]

print("Syntactic Complexity Change values:")
print(syntactic_changes)
print("\nUnique values:", np.unique(syntactic_changes))
print("Number of NaN values:", np.isnan(np.array(syntactic_changes, dtype=float)).sum())

entity_change_ratios = pad_or_truncate([change['change_ratio'] for change in entity_changes], len(ai_score_changes))
sentiment_changes = pad_or_truncate(sentiment_changes, len(ai_score_changes))
coherence_changes = pad_or_truncate(coherence_changes.tolist(), len(ai_score_changes))
diversity_changes = pad_or_truncate(diversity_changes.tolist(), len(ai_score_changes))

correlations = {
    "Semantic Shift": calculate_correlations(semantic_shifts, ai_score_changes),
    "Syntactic Complexity Change": calculate_correlations(syntactic_changes, ai_score_changes),
    "Named Entity Change Ratio": calculate_correlations(entity_change_ratios, ai_score_changes),
    "Sentiment Change": calculate_correlations(sentiment_changes, ai_score_changes),
    "Coherence Change": calculate_correlations(coherence_changes, ai_score_changes),
    "Lexical Diversity Change": calculate_correlations(diversity_changes, ai_score_changes),
}

print("\nCorrelations with AI Detection Score Changes:")
for feature, (correlation, p_value, n) in correlations.items():
    if n < 2:
        print(f"{feature}: Not enough data (only {n} valid samples)")
    else:
        print(f"{feature}: correlation = {correlation:.4f}, p-value = {p_value:.4f} (n = {n})")

feature_values = {
    "Semantic Shift": semantic_shifts,
    "Syntactic Complexity Change": syntactic_changes,
    "Named Entity Change Ratio": entity_change_ratios,
    "Sentiment Change": sentiment_changes,
    "Coherence Change": coherence_changes,
    "Lexical Diversity Change": diversity_changes
}

plt.figure(figsize=(12, 8))
features = list(correlations.keys())
correlation_values = [corr for corr, _, _ in correlations.values()]

sns.barplot(x=correlation_values, y=features)
plt.title("Correlation of Features with AI Detection Score Changes")
plt.xlabel("Spearman Correlation Coefficient")
plt.axvline(x=0, color='r', linestyle='--')
plt.tight_layout()
plt.savefig('/content/drive/My Drive/ai_text_analysis_correlations_original.png')
plt.show()

top_correlations = sorted(correlations.items(), key=lambda x: abs(x[1][0]), reverse=True)[:3]

fig, axes = plt.subplots(1, 3, figsize=(20, 6))
for i, (feature, (correlation, _, _)) in enumerate(top_correlations):
    sns.scatterplot(x=feature_values[feature], y=ai_score_changes, ax=axes[i])
    axes[i].set_title(f"{feature} vs AI Score Change")
    axes[i].set_xlabel(feature)
    axes[i].set_ylabel("AI Detection Score Change")

plt.tight_layout()
plt.savefig('/content/drive/My Drive/ai_text_analysis_top_correlations_original.png')
plt.show()

# Define feature names
feature_names = ["Semantic Shift", "Syntactic Complexity Change", "Named Entity Change Ratio",
                 "Sentiment Change", "Coherence Change", "Lexical Diversity Change"]

valid_indices = [i for i in range(len(ai_score_changes)) if all(arr[i] is not None for arr in [semantic_shifts, syntactic_changes, entity_change_ratios, sentiment_changes, coherence_changes, diversity_changes])]

X = np.column_stack([
    [semantic_shifts[i] for i in valid_indices],
    [syntactic_changes[i] for i in valid_indices],
    [entity_change_ratios[i] for i in valid_indices],
    [sentiment_changes[i] for i in valid_indices],
    [coherence_changes[i] for i in valid_indices],
    [diversity_changes[i] for i in valid_indices]
])

nan_threshold = 0.5  # Remove features with more than 50% NaN values
good_columns = np.isnan(X).sum(axis=0) / X.shape[0] < nan_threshold
X = X[:, good_columns]
feature_names = [name for name, is_good in zip(feature_names, good_columns) if is_good]

y = [ai_score_changes[i] for i in valid_indices]

# Handle NaN values
X = np.array(X)
X[X == None] = np.nan  # Convert None to NaN
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

# Outlier detection
outlier_detector = EllipticEnvelope(contamination=0.1, random_state=42)
outlier_labels = outlier_detector.fit_predict(X_imputed)
X_cleaned = X_imputed[outlier_labels == 1]
y_cleaned = np.array(y)[outlier_labels == 1]

print(f"Removed {sum(outlier_labels == -1)} outliers out of {len(outlier_labels)} samples.")

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=42)

# Check for minimum number of samples
if len(y) < 3:
    print("Error: Not enough samples for meaningful analysis. At least 3 samples are required.")
    sys.exit(1)

# Feature selection
def select_features(X, y):
    selector = RFE(estimator=RandomForestRegressor(n_estimators=100, random_state=42), n_features_to_select=min(10, X.shape[1]))
    selector = selector.fit(X, y)
    return X[:, selector.support_], selector.support_

X_selected, feature_mask = select_features(X_imputed, y)
selected_feature_names = [name for name, selected in zip(feature_names, feature_mask) if selected]

# Define models
models = {
    'Linear Regression': Pipeline([('scaler', StandardScaler()), ('model', LinearRegression())]),
    'Lasso': Pipeline([('scaler', StandardScaler()), ('model', LassoCV(cv=5, max_iter=10000))]),
    'ElasticNet': Pipeline([('scaler', StandardScaler()), ('model', ElasticNetCV(cv=5, max_iter=10000))]),
    'Ridge': Pipeline([('scaler', StandardScaler()), ('model', RidgeCV(cv=5))]),
    'Random Forest': Pipeline([('scaler', StandardScaler()), ('model', RandomForestRegressor(random_state=42))]),
    'Gradient Boosting': Pipeline([('scaler', StandardScaler()), ('model', GradientBoostingRegressor(random_state=42))]),
    'XGBoost': Pipeline([('scaler', StandardScaler()), ('model', XGBRegressor(random_state=42))]),
    'LightGBM': Pipeline([('scaler', StandardScaler()), ('model', LGBMRegressor(random_state=42, min_data_in_leaf=1, min_data_in_bin=1))]),
    'SVR': Pipeline([('scaler', StandardScaler()), ('model', SVR())]),
    'Neural Network': Pipeline([('scaler', StandardScaler()), ('model', MLPRegressor(random_state=42, max_iter=1000))])
}

# Optimize and train models with cross-validation
results = {}
for name, model in models.items():
    if name in ['Random Forest', 'Gradient Boosting', 'XGBoost', 'LightGBM']:
        study = optuna.create_study(direction='minimize')
        study.optimize(lambda trial: objective(trial, model, X_train, y_train), n_trials=100)
        best_params = study.best_params
        if isinstance(model, Pipeline):
            # Check if 'model__' prefix is present in the keys
            if any(k.startswith('model__') for k in best_params):
                model.named_steps['model'].set_params(**{k.split('__')[1]: v for k, v in best_params.items() if k.startswith('model__')})
            else:
                # If 'model__' prefix is not present, assume parameters are for the model directly
                model.named_steps['model'].set_params(**best_params)
        else:
            model.set_params(**best_params)

    # Perform cross-validation
    cv_scores = cross_val_score(model, X_cleaned, y_cleaned, cv=5, scoring='neg_mean_squared_error')
    mse_scores = -cv_scores

    # Fit the model on the entire dataset for feature importance later
    model.fit(X_cleaned, y_cleaned)

    results[name] = {
        'mse': np.mean(mse_scores),
        'mse_std': np.std(mse_scores),
        'r2': r2_score(y_cleaned, model.predict(X_cleaned)),
        'mae': mean_absolute_error(y_cleaned, model.predict(X_cleaned))
    }
    print(f"{name}: MSE = {results[name]['mse']:.4f} (Â±{results[name]['mse_std']:.4f}), R2 = {results[name]['r2']:.4f}, MAE = {results[name]['mae']:.4f}")

# Note about negative R-squared values
if any(result['r2'] < 0 for result in results.values()):
    print("Note: Negative R-squared values indicate that the models are performing poorly, likely due to the small sample size.")

# Ensemble model
base_models = [(name, model) for name, model in models.items() if name in ['Random Forest', 'Gradient Boosting', 'XGBoost', 'LightGBM']]
ensemble = StackingRegressor(estimators=base_models, final_estimator=RidgeCV())
ensemble.fit(X_train, y_train)
y_pred_ensemble = ensemble.predict(X_test)
results['Ensemble'] = {
    'r2': r2_score(y_test, y_pred_ensemble),
    'mse': mean_squared_error(y_test, y_pred_ensemble),
    'mae': mean_absolute_error(y_test, y_pred_ensemble)
}
print(f"Ensemble: R2 = {results['Ensemble']['r2']:.4f}, MSE = {results['Ensemble']['mse']:.4f}, MAE = {results['Ensemble']['mae']:.4f}")

# Select best model
best_model_name = min(results, key=lambda x: results[x]['mse'])
best_model = models[best_model_name]

# Feature importance (adjust based on your best model)
if best_model_name == 'Ensemble':
    # For Ensemble, we'll use permutation importance
    importances = permutation_importance(best_model, X_test, y_test, n_repeats=10, random_state=42).importances_mean
elif hasattr(best_model, 'feature_importances_'):
    importances = best_model.feature_importances_
elif hasattr(best_model, 'named_steps') and hasattr(best_model.named_steps['model'], 'coef_'):
    importances = np.abs(best_model.named_steps['model'].coef_)
else:
    importances = permutation_importance(best_model, X_test, y_test, n_repeats=10, random_state=42).importances_mean

# Normalize importances
importances = importances / np.sum(importances)

# SHAP values (adjust for Ensemble if needed)
if best_model_name == 'Ensemble':
    explainer = shap.Explainer(best_model, X_train)
    shap_values = explainer(X_test)
elif isinstance(best_model, Pipeline):
    explainer = shap.Explainer(best_model.named_steps['model'], best_model.named_steps['scaler'].transform(X_train))
    shap_values = explainer(best_model.named_steps['scaler'].transform(X_test))
else:
    explainer = shap.Explainer(best_model, X_train)
    shap_values = explainer(X_test)

# Visualize feature importances
plt.figure(figsize=(10, 6))
sns.barplot(x=importances, y=selected_feature_names)
plt.title(f"Feature Importances - {best_model_name}")
plt.tight_layout()
plt.savefig('/content/drive/My Drive/ai_text_analysis_feature_importances_original.png')
plt.show()

# SHAP values
if isinstance(best_model, Pipeline):
    explainer = shap.Explainer(best_model.named_steps['model'], best_model.named_steps['scaler'].transform(X_train))
    shap_values = explainer(best_model.named_steps['scaler'].transform(X_test))
else:
    explainer = shap.Explainer(best_model, X_train)
    shap_values = explainer(X_test)

shap.summary_plot(shap_values, X_test, plot_type="bar", feature_names=selected_feature_names)
plt.savefig('/content/drive/My Drive/ai_text_analysis_shap_summary_original.png')
plt.show()

# Partial dependence plots
fig, ax = plt.subplots(figsize=(12, 4))
try:
    PartialDependenceDisplay.from_estimator(best_model, X_test, features=range(min(3, X_test.shape[1])), feature_names=selected_feature_names, ax=ax)
except Exception as e:
    print(f"Could not generate partial dependence plot: {e}")
plt.tight_layout()
plt.savefig('/content/drive/My Drive/ai_text_analysis_partial_dependence_original.png')
plt.close()

# Residual analysis
y_pred = best_model.predict(X_test)
residuals = y_test - y_pred

plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_pred, y=residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.title('Residual Plot')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.savefig('/content/drive/My Drive/ai_text_analysis_residual_plot_original.png')
plt.show()

# QQ plot
fig, ax = plt.subplots(figsize=(10, 6))
stats.probplot(residuals, dist="norm", plot=ax)
ax.set_title("Q-Q plot")
plt.savefig('/content/drive/My Drive/ai_text_analysis_qq_plot_original.png')
plt.show()

# Save comprehensive results
comprehensive_results = []
for idx in range(len(X)):
    result = {
        "prompt": instructions[idx],
        "original_text": original_texts[idx],
        "undetectable_text": undetectable_texts[idx],
        "original_ai_score": original_scores[idx]['ai'],
        "undetectable_ai_score": undetectable_scores[idx]['ai'],
        "ai_score_change": ai_score_changes[idx],
    }
    for name, value in zip(selected_feature_names, X_selected[idx]):
        result[name] = value
    result["predicted_ai_score_change"] = best_model.predict(X_selected[idx].reshape(1, -1))[0]
    comprehensive_results.append(result)

comprehensive_df = pd.DataFrame(comprehensive_results)
comprehensive_csv_path = '/content/drive/My Drive/ai_text_analysis_comprehensive_results_original.csv'
comprehensive_df.to_csv(comprehensive_csv_path, index=False)
print(f"Comprehensive results saved to {comprehensive_csv_path}")

# Summary of findings
print("\nSummary of Findings:")
print(f"1. Best performing model: {best_model_name}")
print(f"2. Best model R-squared: {results[best_model_name]['r2']:.4f}")
print(f"3. Best model Mean Squared Error: {results[best_model_name]['mse']:.4f}")
print(f"4. Best model Mean Absolute Error: {results[best_model_name]['mae']:.4f}")
print(f"5. Top correlated feature with AI detection score changes: {top_correlations[0][0]}")
print("6. Feature importances:")
for name, importance in zip(selected_feature_names, importances):
    print(f"   - {name}: {importance:.4f}")
print("7. Visualizations saved:")
print("   - Correlation plot: ai_text_analysis_correlations_original.png")
print("   - Top correlations scatter plot: ai_text_analysis_top_correlations_original.png")
print("   - Feature importances plot: ai_text_analysis_feature_importances_original.png")
print("   - SHAP summary plot: ai_text_analysis_shap_summary_original.png")
print("   - Partial dependence plot: ai_text_analysis_partial_dependence_original.png")
print("   - Residual plot: ai_text_analysis_residual_plot_original.png")
print("   - Q-Q plot: ai_text_analysis_qq_plot_original.png")
print(f"8. Comprehensive results saved to: {comprehensive_csv_path}")
